#+TODO: TODO DOING | DONE

wip notes:
- RocksDB-level metrics would be useful

!! Replica set failover causes any in-flight requests to time out (and get dropped)
!! RPC error handling is bad. Need to have a way to serialize app-level errors across the boundary

Thoughts on "Leased" operations:
- Must skip raft loop for acceptable perf
- need to handle failover case. Shard failover could cause "lost updates" if we skip raft
- Gain a lease to a specific shard in the replica set. This shard serves all read traffic?
- can clients read from a replica? ES seems to think this is OK
- if so, still need to obtain a lease to know when the replica is fully up to date
- Ultimately, need to guarantee all reads after a refresh are consistent

* Current Notes
  
  Missing Validation:
  - Internal mappings fields

* Release Tasks
  
** 0.1 [50%]
  :PROPERTIES:
  :COOKIE_DATA: todo recursive
  :END:
*** Search [4/4] 
**** DONE Term Query
     CLOSED: [2019-01-11 Fri 11:43]
**** DONE Bool Query
     CLOSED: [2019-01-11 Fri 11:45]
**** DONE Respond with document information
**** DONE match_all
     CLOSED: [2019-05-25 Sat 10:13]
*** APIs [6/9]
**** DONE Get Index
     CLOSED: [2019-01-11 Fri 23:15]
**** DONE Refresh API
     CLOSED: [2019-01-14 Mon 23:12]
**** DONE Get Document by ID
     CLOSED: [2019-01-21 Mon 09:35]
**** DONE Delete
     CLOSED: [2019-01-24 Thu 22:27]
**** DONE Bulk API
     CLOSED: [2019-06-27 Thu 22:46]
**** TODO Cluster Health API
**** TODO Proper errors
**** DONE Index Exists
     CLOSED: [2019-06-28 Fri 10:27]
*** Clustering [5/7]
**** DONE Allow nodes to join without joining the master raft group
     CLOSED: [2019-01-20 Sun 11:28]
**** DOING Allocate unallocated shards when a node joins the cluster
**** DONE Detect when a node leaves the cluster
     CLOSED: [2019-01-20 Sun 10:29]
**** TODO De-allocate shard when a node leaves the cluster
**** DONE Remove inter-node polling
     CLOSED: [2019-06-23 Sun 23:30]
     - Gossip shard/index routing information
**** DONE Gossip infrastructure - Discovery
     CLOSED: [2019-01-18 Fri 08:10]
**** DONE Cache shard routing information
     CLOSED: [2019-06-23 Sun 23:30]
     - Done for indices. Need to figure out when to refresh it and do it for shard-level routing
     - Update local cache from gossips
     - Check meta layer synchronously if cache missed / was wrong
*** Indexing [3/5]
**** DONE Commit on an interval instead of per-operation
     CLOSED: [2019-01-14 Mon 08:26]
**** DONE Upsert semantics
     CLOSED: [2019-01-26 Sat 10:17]

**** DONE Create semantics
     CLOSED: [2019-01-26 Sat 10:03]
     keep track of indexed versions
     when commit(), trim the in-memory map
     when looking up, check the map for a version. If absent, use a searcher
**** TODO Optimistic Locking
     Deletes are hard - elasticsearch uses a time window
**** TODO Response format
*** Schema [1/1]
**** DONE Basic data types
     CLOSED: [2019-01-11 Fri 17:02]
     - Strings
     - Longs
     - Objects
*** Operations [1/5]
**** TODO Terraform configuration
**** TODO Deploy cluster to AWS
**** TODO Baseline perf test
**** TODO Structured logging
**** DONE Expose prometheus metrics
     CLOSED: [2019-07-05 Fri 11:58]
*** Storage [1/2]
**** TODO Raft Log Compaction
**** DONE Indexing should be async, not in raft loop
     CLOSED: [2019-07-05 Fri 11:49]
*** Tests [1/7]
**** DONE Search Storage
     CLOSED: [2019-01-26 Sat 10:22]
**** TODO Integration
**** TODO Raft Storage
**** TODO Node Router
**** TODO Gossip
**** TODO Web API
**** TODO Meta State Machine
**** TODO Search State Machine
*** Polish [1/6]
**** DONE Reasonable test suite (broken out)
     CLOSED: [2019-01-23 Wed 08:46]
**** TODO CI
**** TODO pre-commit script
**** TODO Figure out license
**** TODO README
**** TODO Contributor guidelines
** 0.2 [0%]
  :PROPERTIES:
  :COOKIE_DATA: todo recursive
  :END:
*** Search [0/5]
**** TODO Specify document count limit
**** TODO Sepcify document offset
**** TODO Sorting
**** TODO Aggregations
*** API [0/5]
**** TODO Multi Get
**** TODO Get Mapping
**** TODO Update index settings
**** TODO API docs
**** TODO Per-shard response information
**** TODO Nodes cat API
*** Indexing [0/4]
**** TODO Partial Failure in bulk requests
**** TODO Update?
*** Schema [0/13]
**** TODO Text
**** TODO Float
**** TODO Boolean
**** TODO Dates
**** TODO DateTimes
**** TODO Integer
**** TODO Short
**** TODO Byte
**** TODO Double
**** TODO Float
**** TODO Half Float
**** TODO Scaled Float
**** TODO Binary
*** Clustering [0/4]
**** TODO Replica sets should have a lease holder
    - https://github.com/cockroachdb/cockroach/blob/master/docs/design.md#range-leases
      https://github.com/cockroachdb/cockroach/blob/master/docs/RFCS/20160210_range_leases.md
**** TODO Direct all reads at the lease holder
**** TODO Heartbeat at the node level, not the shard level
**** TODO Remove 'node id' configuration 
     Shouldn't have to force the user to configure this, we should
     figure out node ids from address or something else
** 0.3 [0%]
  :PROPERTIES:
  :COOKIE_DATA: todo recursive
  :END:
*** Search [0/2]
**** TODO Cross-index search
*** API [0/1]
**** TODO Update mappings
**** TODO Cluster settings API
*** Indexing [/]
*** Percolation [0/1]
**** TODO Implment it
*** Schema [0/3]
**** TODO Nested Documents
**** TODO Range Types?
*** Clustering [0/3]
**** TODO Split / Merge(?)
**** TODO Auto-scale replication
* Icebox

** Tooling
*** tower-grpc is interesting, but ergonomics of pingcap's grpc are currently better. Both suck.
    
** Tantivy Issues
   - Max field count is 255
   - Threading is weird. I want to manage the thread pool across many indices
   - No way to have a user-defined doc id

** Neat Ideas
   - Avoid dirty reads!
   - Can we provide even better consistency guarantees?
   - Joins! 
     - https://www.memsql.com/blog/scaling-distributed-joins/
   - Autoscaling
     - Split shards at certain conditions
     - Add replicas at certain conditions
* Impl Notes
  Elasticsearch ids: https://github.com/elastic/elasticsearch/blob/master/server/src/main/java/org/elasticsearch/index/mapper/Uid.java#L178
